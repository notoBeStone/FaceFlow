//
//  IdentificationViewModel.swift
//  HackWords
//
//  Created by Claude on 2025/10/13.
//

import Foundation
import SwiftUI
import Vision
import VisionKit
import CoreImage
import CoreVideo

@MainActor
class IdentificationViewModel: ObservableObject {
    let originalImage: UIImage

    @Published var currentImage: UIImage?
    @Published var statusText: String = "Processing"
    @Published var identifiedWord: String?
    @Published var isProcessing: Bool = false

    // è¯†åˆ«çŠ¶æ€
    enum IdentificationState {
        case processing    // åˆå§‹æ€ï¼šå±•ç°å®Œæ•´å›¾ç‰‡ï¼Œæ–‡æ¡ˆä¸º Processing
        case identifying   // è¯†åˆ«æ€ï¼šç»è¿‡æŠ å›¾APIï¼Œè·å¾—ä¸»è¦ç›®æ ‡ç‰©ä½“ï¼Œæ–‡æ¡ˆä¸º Identifying
        case optimizing    // ä¼˜åŒ–æ€ï¼šè¿›è¡Œæ™ºèƒ½è£å‰ªå’Œè¾¹ç¼˜å¹³æ»‘å¤„ç†ï¼Œæ–‡æ¡ˆä¸º Optimizing
        case completed     // å®Œæˆæ€ï¼šä»LLM APIè·å¾—å›¾ç‰‡å¯¹åº”çš„å•è¯ï¼Œæ–‡æ¡ˆå±•ç¤ºä¸ºå¯¹åº”çš„å•è¯
    }

    @Published var currentState: IdentificationState = .processing

    init(image: UIImage) {
        self.originalImage = image
        self.currentImage = image
    }

    // å¼€å§‹è¯†åˆ«æµç¨‹
    func startIdentification() {
        isProcessing = true
        currentState = .processing
        statusText = "Processing"

        debugPrint("ğŸ¯ å¼€å§‹è¯†åˆ«æµç¨‹ï¼ŒåŸå›¾å°ºå¯¸: \(originalImage.size)")

        // æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
            self.performSubjectSegmentation()
        }
    }

    // æ‰§è¡Œä¸»ä½“åˆ†å‰²ï¼ˆæŠ å›¾ï¼‰
    private func performSubjectSegmentation() {
        currentState = .identifying
        statusText = "Identifying"

        debugPrint("ğŸ” å¼€å§‹æ‰§è¡Œä¸»ä½“åˆ†å‰²")

        // æ£€æŸ¥æ˜¯å¦æ”¯æŒVisionKitçš„æŠ å›¾åŠŸèƒ½
        if #available(iOS 17.0, *) {
            performVisionKitSegmentation()
        } else {
            debugPrint("âŒ iOSç‰ˆæœ¬ä¸æ”¯æŒVisionKitæŠ å›¾")
            // å¦‚æœä¸æ”¯æŒï¼Œç›´æ¥å®Œæˆè¯†åˆ«
            completeIdentificationWithoutLLM()
        }
    }

    // ä½¿ç”¨VisionKitè¿›è¡ŒæŠ å›¾
    private func performVisionKitSegmentation() {
        guard let cgImage = originalImage.cgImage else {
            debugPrint("âŒ æ— æ³•è·å–CGImage")
            completeIdentificationWithoutLLM()
            return
        }

        debugPrint("âœ… å¼€å§‹VisionKitæŠ å›¾ï¼Œå›¾ç‰‡å°ºå¯¸: \(originalImage.size)")
        debugPrint("ğŸ“Š CGImage info: width=\(cgImage.width), height=\(cgImage.height), colorSpace=\(cgImage.colorSpace?.name as String? ?? "unknown")")

        let request = VNGenerateForegroundInstanceMaskRequest { request, error in
            debugPrint("ğŸ“¨ VisionKitè¯·æ±‚å›è°ƒå¼€å§‹")

            if let error = error {
                debugPrint("âŒ æŠ å›¾å¤±è´¥: \(error.localizedDescription)")
                DispatchQueue.main.async {
                    self.completeIdentificationWithoutLLM()
                }
                return
            }

            guard let results = request.results else {
                debugPrint("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»“æœ")
                DispatchQueue.main.async {
                    self.completeIdentificationWithoutLLM()
                }
                return
            }

            debugPrint("ğŸ“Š æ‰¾åˆ° \(results.count) ä¸ªç»“æœ")

            guard let result = results.first as? VNInstanceMaskObservation else {
                debugPrint("âŒ ç¬¬ä¸€ä¸ªç»“æœä¸æ˜¯VNInstanceMaskObservation")
                DispatchQueue.main.async {
                    self.completeIdentificationWithoutLLM()
                }
                return
            }

            debugPrint("âœ… æ‰¾åˆ°æŠ å›¾ç»“æœï¼Œmask instances: \(result.allInstances.count)")

            do {
                // åˆ›å»ºæŠ å›¾åçš„å›¾ç‰‡å¹¶è¿›è¡Œä¼˜åŒ–å¤„ç†
                let optimizedImage = try self.createOptimizedMaskedImage(from: result, originalImage: cgImage)
                DispatchQueue.main.async {
                    debugPrint("âœ… ä¼˜åŒ–æŠ å›¾å›¾ç‰‡åˆ›å»ºæˆåŠŸï¼Œæ–°å›¾ç‰‡å°ºå¯¸: \(optimizedImage.size)")
                    self.currentImage = optimizedImage

                    // æŠ å›¾å®Œæˆåè¿›è¡ŒLLMè¯†åˆ«
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                        self.performLLMIdentification()
                    }
                }
            } catch {
                debugPrint("âŒ åˆ›å»ºæŠ å›¾å¤±è´¥: \(error.localizedDescription)")
                DispatchQueue.main.async {
                    self.completeIdentificationWithoutLLM()
                }
            }
        }

        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        do {
            try handler.perform([request])
            debugPrint("âœ… VisionKitè¯·æ±‚å·²å‘é€")
        } catch {
            debugPrint("âŒ VisionKitè¯·æ±‚å¤±è´¥: \(error.localizedDescription)")
            completeIdentificationWithoutLLM()
        }
    }

    // æš‚æ—¶è·³è¿‡LLMè¯†åˆ«ï¼Œç›´æ¥å®Œæˆè¯†åˆ«æµç¨‹
    private func completeIdentificationWithoutLLM() {
        debugPrint("ğŸ å®Œæˆè¯†åˆ«æµç¨‹ï¼ˆè·³è¿‡LLMï¼‰")
        currentState = .completed
        identifiedWord = "test_word"
        statusText = "test_word"
        isProcessing = false
    }

    // åˆ›å»ºä¼˜åŒ–åçš„æŠ å›¾å›¾ç‰‡ - é›†æˆæ™ºèƒ½è£å‰ªå’Œè¾¹ç¼˜å¹³æ»‘
    private func createOptimizedMaskedImage(from maskObservation: VNInstanceMaskObservation, originalImage: CGImage) throws -> UIImage {
        let startTime = CFAbsoluteTimeGetCurrent()

        // æ›´æ–°çŠ¶æ€ä¸ºä¼˜åŒ–å¤„ç†
        DispatchQueue.main.async {
            self.currentState = .optimizing
            self.statusText = "Optimizing"
        }

        debugPrint("ğŸš€ å¼€å§‹åˆ›å»ºä¼˜åŒ–æŠ å›¾å›¾ç‰‡")

        // 1. åˆ›å»ºåŸºç¡€æŠ å›¾å›¾ç‰‡
        let baseMaskedImage = try createMaskedImage(from: maskObservation, originalImage: originalImage)
        debugPrint("âœ… åŸºç¡€æŠ å›¾å®Œæˆï¼Œå°ºå¯¸: \(baseMaskedImage.size)")

        var optimizedImage = baseMaskedImage
        let originalUIImage = UIImage(cgImage: originalImage)
        let originalSize = originalUIImage.size

        var wasCropped = false
        var wasSmoothed = false

        // 2. æ™ºèƒ½è£å‰ªï¼ˆåŸºäº mask çš„æœ€å°çŸ©å½¢åŒºåŸŸï¼‰
        let croppedImage = MaskCroppingUtil.cropImageBasedOnMask(
            maskObservation: maskObservation,
            originalImage: originalUIImage,
            croppedImage: optimizedImage
        )

        // æ£€æŸ¥æ˜¯å¦çœŸçš„è¿›è¡Œäº†è£å‰ª
        if croppedImage.size != optimizedImage.size {
            optimizedImage = croppedImage
            wasCropped = true
            debugPrint("âœ‚ï¸ æ™ºèƒ½è£å‰ªå®Œæˆï¼Œæ–°å°ºå¯¸: \(optimizedImage.size)")
        } else {
            debugPrint("â­ï¸ è·³è¿‡æ™ºèƒ½è£å‰ªï¼ˆæœªæ‰¾åˆ°æœ‰æ•ˆçš„è£å‰ªåŒºåŸŸï¼‰")
        }

        // 3. è¾¹ç¼˜å¹³æ»‘å¤„ç†ï¼ˆè½»åº¦æŠ—é”¯é½¿ï¼‰- æš‚æ—¶ç¦ç”¨
        debugPrint("â­ï¸ è¾¹ç¼˜å¹³æ»‘å¤„ç†æš‚æ—¶ç¦ç”¨")
        wasSmoothed = false

        let totalDuration = CFAbsoluteTimeGetCurrent() - startTime
        debugPrint("ğŸ‰ ä¼˜åŒ–æŠ å›¾å›¾ç‰‡åˆ›å»ºå®Œæˆï¼Œæ€»è€—æ—¶: \(String(format: "%.3f", totalDuration))s")
        debugPrint("ğŸ“ æœ€ç»ˆå°ºå¯¸: \(optimizedImage.size)")

        // è®°å½•æ€§èƒ½ç»Ÿè®¡
        DispatchQueue.main.async {
            self.recordPerformanceStats(
                originalSize: originalSize,
                finalSize: optimizedImage.size,
                processingTime: totalDuration,
                wasCropped: wasCropped,
                wasSmoothed: wasSmoothed
            )
        }

        return optimizedImage
    }

    // åˆ›å»ºæŠ å›¾åçš„å›¾ç‰‡ - ä½¿ç”¨è‡ªå®šä¹‰shaderå¤„ç†
    private func createMaskedImage(from maskObservation: VNInstanceMaskObservation, originalImage: CGImage) throws -> UIImage {
        debugPrint("ğŸ¨ å¼€å§‹ä½¿ç”¨è‡ªå®šä¹‰shaderåˆ›å»ºæŠ å›¾å›¾ç‰‡")

        let pixelBuffer = try maskObservation.generateMaskedImage(ofInstances: [0], from: VNImageRequestHandler(cgImage: originalImage, options: [:]), croppedToInstancesExtent: false)

        debugPrint("ğŸ“Š PixelBufferåˆ›å»ºæˆåŠŸ: width=\(CVPixelBufferGetWidth(pixelBuffer)), height=\(CVPixelBufferGetHeight(pixelBuffer))")

        // åˆ›å»ºCore Image contextï¼Œä½¿ç”¨GPUåŠ é€Ÿ
        let context = CIContext(options: [.useSoftwareRenderer: false, .priorityRequestLow: false])

        // å°†åŸå§‹å›¾ç‰‡è½¬æ¢ä¸ºCIImageï¼Œç»Ÿä¸€è½¬æ¢ä¸ºRGBè‰²å½©ç©ºé—´
        let originalCIImage = CIImage(cgImage: originalImage)
        let rgbOriginalImage = originalCIImage.applyingFilter("CIColorControls", parameters: [
            kCIInputSaturationKey: 1.0,
            kCIInputBrightnessKey: 0.0,
            kCIInputContrastKey: 1.0
        ])
        debugPrint("âœ… åŸå§‹å›¾ç‰‡è½¬æ¢ä¸ºRGB CIImageæˆåŠŸ")

        // å°†maskè½¬æ¢ä¸ºCIImage
        let maskCIImage = CIImage(cvPixelBuffer: pixelBuffer)
        debugPrint("âœ… maskè½¬æ¢ä¸ºCIImageæˆåŠŸï¼Œextent: \(maskCIImage.extent)")

        // ä½¿ç”¨è‡ªå®šä¹‰shaderè¿›è¡ŒæŠ å›¾
        let maskedImage = try applyMaskShader(rgbOriginalImage, mask: maskCIImage, context: context)
        debugPrint("âœ… shaderæŠ å›¾å¤„ç†æˆåŠŸ")

        // å°†ç»“æœè½¬å›UIImage
        guard let outputCGImage = context.createCGImage(maskedImage, from: maskedImage.extent) else {
            debugPrint("âŒ æ— æ³•åˆ›å»ºæœ€ç»ˆCGImage")
            throw NSError(domain: "FinalImageCreation", code: 2, userInfo: [NSLocalizedDescriptionKey: "Failed to create final CGImage"])
        }

        let result = UIImage(cgImage: outputCGImage)
        debugPrint("ğŸ‰ è‡ªå®šä¹‰shaderæŠ å›¾å®Œæˆï¼Œæœ€ç»ˆå°ºå¯¸: \(result.size)")

        return result
    }

    // ä½¿ç”¨è‡ªå®šä¹‰shaderåº”ç”¨maskï¼šåŸå›¾å‡å»maskå¾—åˆ°ä¸»ä½“
    private func applyMaskShader(_ image: CIImage, mask: CIImage, context: CIContext) throws -> CIImage {
        // åˆ›å»ºè‡ªå®šä¹‰kernel
        let kernel = CIKernel(source: """
        kernel vec4 maskShader(__sample image, __sample mask) {
            // è·å–å½“å‰åæ ‡çš„åƒç´ é¢œè‰²
            vec4 imageColor = image;
            vec4 maskColor = mask;

            // å¦‚æœmaskåƒç´ æ˜¯é€æ˜çš„ï¼ˆalpha=0ï¼‰ï¼Œè¯´æ˜è¿™æ˜¯ä¸»ä½“åŒºåŸŸï¼Œä¿ç•™åŸå›¾
            // å¦‚æœmaskåƒç´ ä¸é€æ˜ï¼ˆalpha>0ï¼‰ï¼Œè¯´æ˜è¿™æ˜¯èƒŒæ™¯åŒºåŸŸï¼Œå˜ä¸ºé€æ˜
            float alpha = 1.0 - maskColor.a;

            // è¿”å›åŸå›¾é¢œè‰²ï¼Œä½†alphaæ ¹æ®maskçš„é€æ˜åº¦è°ƒæ•´
            return vec4(imageColor.rgb, alpha);
        }
        """)

        guard let shader = kernel else {
            throw NSError(domain: "ShaderCreation", code: 1, userInfo: [NSLocalizedDescriptionKey: "Failed to create custom shader"])
        }

        // åº”ç”¨shader
        guard let maskedImage = shader.apply(extent: image.extent, roiCallback: { (index, rect) in
            return rect
        }, arguments: [image, mask]) else {
            throw NSError(domain: "ShaderApplication", code: 1, userInfo: [NSLocalizedDescriptionKey: "Failed to apply custom shader"])
        }

        return maskedImage
    }

    // å°†CVPixelBufferè½¬æ¢ä¸ºCGImageçš„è¾…åŠ©æ–¹æ³•
    private func createCGImage(from pixelBuffer: CVPixelBuffer) -> CGImage? {
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        let context = CIContext()
        return context.createCGImage(ciImage, from: ciImage.extent)
    }

    // è¿›è¡ŒLLMè¯†åˆ«
    private func performLLMIdentification() {
        // è·å–ç”¨æˆ·çš„ç›®æ ‡å­¦ä¹ è¯­è¨€
        let targetLanguage = OnboardingManager.shared.currentOnboardingData?.targetLanguage ?? "english"

        Task {
            do {
                let word = try await identifyImageWithLLM(image: currentImage ?? originalImage, targetLanguage: targetLanguage)

                await MainActor.run {
                    self.currentState = .completed
                    self.identifiedWord = word
                    self.statusText = word
                    self.isProcessing = false
                }
            } catch {
                await MainActor.run {
                    self.currentState = .completed
                    self.identifiedWord = "è¯†åˆ«å¤±è´¥"
                    self.statusText = "è¯†åˆ«å¤±è´¥"
                    self.isProcessing = false
                }
            }
        }
    }

    // ä½¿ç”¨LLMè¯†åˆ«å›¾ç‰‡
    private func identifyImageWithLLM(image: UIImage, targetLanguage: String) async throws -> String {
        debugPrint("ğŸš€ å¼€å§‹LLMè¯†åˆ«æµç¨‹")

        // ä¸Šä¼ å›¾ç‰‡åˆ°S3
        let imageUrl = try await uploadImageToS3(image: image)
        debugPrint("âœ… å›¾ç‰‡ä¸Šä¼ æˆåŠŸï¼ŒURL: \(imageUrl)")

        // æ„é€ æ¶ˆæ¯ - ç³»ç»ŸæŒ‡ä»¤å’Œå›¾ç‰‡åˆ†åˆ«åœ¨ä¸åŒçš„æ¶ˆæ¯ä¸­
        let systemMessage = ChatGPTMessage(messageId: 1, role: GPTRole.system, content: "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾ç‰‡è¯†åˆ«åŠ©æ‰‹ã€‚è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„ä¸»è¦ç‰©ä½“ï¼Œå¹¶è¿”å›å¯¹åº”çš„\(targetLanguage)å•è¯ã€‚åªéœ€è¦è¿”å›ä¸€ä¸ªå•è¯ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–è§£é‡Šã€‚", imageUrl: nil)

        // å›¾ç‰‡æ¶ˆæ¯ï¼Œä½¿ç”¨URLè€Œä¸æ˜¯base64
        let imageMessage = ChatGPTMessage(messageId: 2, role: GPTRole.user, content: "", imageUrl: imageUrl)

        // æ–‡æœ¬æŒ‡ä»¤æ¶ˆæ¯
        let textMessage = ChatGPTMessage(messageId: 3, role: GPTRole.user, content: "è¯·è¯†åˆ«è¿™å¼ å›¾ç‰‡ä¸­çš„ä¸»è¦ç‰©ä½“ï¼Œè¿”å›å¯¹åº”çš„\(targetLanguage)å•è¯ã€‚", imageUrl: nil)

        debugPrint("ğŸ“¨ å‘é€LLMè¯·æ±‚ï¼Œå›¾ç‰‡URL: \(imageUrl)")

        // è°ƒç”¨LLM API
        let response = try await TemplateAPI.ChatGPT.llmCompletion(
            [systemMessage, imageMessage, textMessage],
            configuration: GPTConfig.default,
            responseFormat: nil
        )

        debugPrint("âœ… LLMå“åº”: \(response)")

        // æ¸…ç†å“åº”ï¼Œåªä¿ç•™å•è¯
        let cleanedWord = response.trimmingCharacters(in: .whitespacesAndNewlines)
        return cleanedWord.isEmpty ? "unknown" : cleanedWord
    }

    // ä¸Šä¼ å›¾ç‰‡åˆ°S3
    private func uploadImageToS3(image: UIImage) async throws -> String {
        debugPrint("ğŸ“¤ å¼€å§‹ä¸Šä¼ å›¾ç‰‡åˆ°S3")

        // å°†å›¾ç‰‡è½¬æ¢ä¸ºJPEGæ•°æ®
        guard let imageData = image.jpegData(compressionQuality: 0.8) else {
            throw NSError(domain: "ImageConversion", code: 1, userInfo: [NSLocalizedDescriptionKey: "Failed to convert image to JPEG"])
        }

        debugPrint("ğŸ“Š å›¾ç‰‡æ•°æ®å¤§å°: \(imageData.count) bytes")

        // ä½¿ç”¨TemplateAPIçš„S3ä¸Šä¼ åŠŸèƒ½
        let imageUrl = try await TemplateAPI.S3.upload(data: imageData, fileExtension: "jpg")
        debugPrint("âœ… S3ä¸Šä¼ æˆåŠŸ: \(imageUrl)")

        return imageUrl
    }

    // MARK: - æ€§èƒ½ç›‘æ§

    /// è®°å½•å¤„ç†æ€§èƒ½ç»Ÿè®¡
    private func recordPerformanceStats(
        originalSize: CGSize,
        finalSize: CGSize,
        processingTime: TimeInterval,
        wasCropped: Bool,
        wasSmoothed: Bool
    ) {
        let compressionRatio = (finalSize.width * finalSize.height) / (originalSize.width * originalSize.height)
        let spaceSaved = 1.0 - compressionRatio

        let stats: [String: Any] = [
            "originalSize": "\(originalSize)",
            "finalSize": "\(finalSize)",
            "processingTime": String(format: "%.3f", processingTime),
            "compressionRatio": String(format: "%.2f", compressionRatio),
            "spaceSaved": String(format: "%.1f", spaceSaved * 100),
            "wasCropped": wasCropped,
            "wasSmoothed": wasSmoothed,
            "timestamp": Date().timeIntervalSince1970
        ]

        debugPrint("ğŸ“Š æ€§èƒ½ç»Ÿè®¡: \(stats)")

        // å¯ä»¥å°†ç»Ÿè®¡ä¿¡æ¯å‘é€åˆ°åˆ†ææœåŠ¡
        // TemplateAPI.Analytics.track("image_optimization_performance", parameters: stats)
    }

    /// è·å–å½“å‰å¤„ç†æ€§èƒ½ä¿¡æ¯
    func getPerformanceInfo() -> [String: Any] {
        let maskStats = MaskCroppingUtil.getPerformanceStats()
        let smoothingStats = EdgeSmoothingUtil.getPerformanceStats()

        return [
            "maskCropping": maskStats,
            "edgeSmoothing": smoothingStats,
            "supportedFeatures": [
                "ios17_minimum": true,
                "fast_cropping": true,
                "light_smoothing": true,
                "gpu_acceleration": true
            ]
        ]
    }

    // ä¿å­˜åˆ°å•è¯æœ¬
    func saveToWordbook() {
        guard let word = identifiedWord, !word.isEmpty && word != "è¯†åˆ«å¤±è´¥" else {
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å•è¯å¯ä»¥ä¿å­˜")
            return
        }

        // è·å–å½“å‰å¤„ç†åçš„å›¾ç‰‡ï¼ˆæŠ å›¾åçš„å›¾ç‰‡ï¼‰æˆ–åŸå›¾
        let imageToSave = currentImage ?? originalImage

        // è·å–ç”¨æˆ·çš„ç›®æ ‡å­¦ä¹ è¯­è¨€
        let targetLanguage = OnboardingManager.shared.currentOnboardingData?.targetLanguage ?? "english"

        debugPrint("ğŸ’¾ å¼€å§‹ä¿å­˜å•è¯åˆ°å•è¯æœ¬: \(word)")
        debugPrint("ğŸ“¸ ä½¿ç”¨å›¾ç‰‡: \(imageToSave.size)")
        debugPrint("ğŸŒ ç›®æ ‡è¯­è¨€: \(targetLanguage)")

        // ä½¿ç”¨WordBookManagerä¿å­˜å•è¯
        let success = WordBookManager.shared.createWord(
            image: imageToSave,
            targetLanguage: targetLanguage,
            word: word
        ) != nil

        if success {
            debugPrint("âœ… å•è¯ä¿å­˜æˆåŠŸ: \(word)")
        } else {
            debugPrint("âŒ å•è¯ä¿å­˜å¤±è´¥: \(word)")
        }
    }
}
